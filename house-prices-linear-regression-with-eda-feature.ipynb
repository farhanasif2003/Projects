{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🏡 House Prices Prediction – Linear Regression with EDA, Normal Equation, SGD, and RMSE Evaluation\n\n### 📋 Project Overview\n\nThis notebook presents a full end-to-end regression pipeline to predict house prices based on the **Ames Housing Dataset**.  \nWe apply both **closed-form** and **iterative** modeling approaches to build a reliable, well-evaluated model.\n\n---\n\n### ✅ Key Workflow Steps:\n- 🔍 **Exploratory Data Analysis (EDA)** to understand the data structure and identify key drivers of price\n- 🧹 **Data Preprocessing**: \n  - Handling missing values\n  - Outlier detection & removal\n  - Log-transforming the target (`SalePrice`)\n- 🧠 **Feature Engineering**:\n  - Creating new features like `TotalBathrooms`, `HouseAge`, and `HasBasement`\n- 📈 **Model Training**:\n  - **LinearRegression (Normal Equation)** – used for final prediction\n  - **SGDRegressor** with `StandardScaler` – used for iterative RMSE visualization and evaluation\n- 📊 **Evaluation**:\n  - R² and RMSE on train/validation splits\n  - RMSE plotted over epochs to analyze learning behavior\n- 📤 **Submission**:\n  - Final predictions converted back from log scale\n  - Submission file created in the correct format\n\n---\n\n### 🛠️ Tools & Libraries Used:\n- Python · Pandas · NumPy · Matplotlib · Seaborn  \n- Scikit-learn (LinearRegression, SGDRegressor, StandardScaler, Pipelines)\n\n---\n\n> This notebook reflects a structured ML workflow with explainable modeling, proper evaluation, and clean documentation — aligned with best practices for applied regression modeling.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## 1. Import Libraries & Data Loading","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn import datasets\nfrom sklearn import pipeline, preprocessing\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn import model_selection\n\nimport pandas as pd\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:55.6743Z","iopub.execute_input":"2025-04-20T10:45:55.674637Z","iopub.status.idle":"2025-04-20T10:45:57.702869Z","shell.execute_reply.started":"2025-04-20T10:45:55.674613Z","shell.execute_reply":"2025-04-20T10:45:57.702029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define plot settings","metadata":{}},{"cell_type":"code","source":"# define plt settings (you don't have to do it every time, you can you the default values)\nplt.rcParams[\"font.size\"] = 20\nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams[\"xtick.labelsize\"] = 20\nplt.rcParams[\"ytick.labelsize\"] = 20\nplt.rcParams[\"legend.fontsize\"] = 20\nplt.rcParams[\"figure.figsize\"] = (20,10)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:57.704174Z","iopub.execute_input":"2025-04-20T10:45:57.704606Z","iopub.status.idle":"2025-04-20T10:45:57.710658Z","shell.execute_reply.started":"2025-04-20T10:45:57.704583Z","shell.execute_reply":"2025-04-20T10:45:57.709518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Import The Dataset","metadata":{}},{"cell_type":"code","source":"train_data_path = '/kaggle/input/house-prices-advanced-regression-techniques/train.csv'\ntest_data_path = '/kaggle/input/house-prices-advanced-regression-techniques/test.csv'\ntrain_df = pd.read_csv(train_data_path)\ntest_df = pd.read_csv(test_data_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:57.711676Z","iopub.execute_input":"2025-04-20T10:45:57.712144Z","iopub.status.idle":"2025-04-20T10:45:57.798621Z","shell.execute_reply.started":"2025-04-20T10:45:57.712113Z","shell.execute_reply":"2025-04-20T10:45:57.797712Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Initial Data Review","metadata":{}},{"cell_type":"code","source":"print(\"train_df shape:\", train_df.shape)\nprint(\"test_df shape:\", test_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:57.800593Z","iopub.execute_input":"2025-04-20T10:45:57.800874Z","iopub.status.idle":"2025-04-20T10:45:57.806094Z","shell.execute_reply.started":"2025-04-20T10:45:57.800851Z","shell.execute_reply":"2025-04-20T10:45:57.805241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:57.806976Z","iopub.execute_input":"2025-04-20T10:45:57.807231Z","iopub.status.idle":"2025-04-20T10:45:57.874128Z","shell.execute_reply.started":"2025-04-20T10:45:57.807211Z","shell.execute_reply":"2025-04-20T10:45:57.87333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:57.875011Z","iopub.execute_input":"2025-04-20T10:45:57.875232Z","iopub.status.idle":"2025-04-20T10:45:57.906281Z","shell.execute_reply.started":"2025-04-20T10:45:57.875214Z","shell.execute_reply":"2025-04-20T10:45:57.904835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:57.907373Z","iopub.execute_input":"2025-04-20T10:45:57.907748Z","iopub.status.idle":"2025-04-20T10:45:58.015952Z","shell.execute_reply.started":"2025-04-20T10:45:57.907691Z","shell.execute_reply":"2025-04-20T10:45:58.014779Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Handling Missing Values","metadata":{}},{"cell_type":"code","source":"def show_missing_values(df, show_top=20, plot=False, return_df=False):\n    \"\"\"\n    Displays and optionally plots missing values in a DataFrame.\n\n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The dataset to analyze.\n    show_top : int\n        Number of top missing columns to display.\n    plot : bool\n        If True, plots a horizontal bar chart of missing value percentages.\n    return_df : bool\n        If True, returns the missing values summary DataFrame.\n\n    Returns:\n    --------\n    pd.DataFrame (optional)\n        A summary of missing values (total and percent).\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Expected a pandas DataFrame.\")\n\n    total_missing = df.isnull().sum()\n    percent_missing = (total_missing / len(df)) * 100\n\n    missing_df = pd.concat([total_missing, percent_missing], axis=1)\n    missing_df.columns = ['Total Missing', 'Percent Missing']\n    missing_df = missing_df[missing_df['Total Missing'] > 0].sort_values('Total Missing', ascending=False)\n\n    print(\"Missing Values (Top {}):\\n\".format(show_top))\n    display(missing_df.head(show_top))\n\n    if plot and not missing_df.empty:\n        missing_df.head(show_top).sort_values(\"Percent Missing\").plot(\n            kind=\"barh\", figsize=(10, 6), color=\"salmon\", edgecolor=\"black\"\n        )\n        plt.title(\"Top Missing Values\")\n        plt.xlabel(\"Percent Missing\")\n        plt.ylabel(\"Features\")\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()\n\n    if return_df:\n        return missing_df\n\ndef handle_missing_values(df, numeric_strategy='median', categorical_fill='None'):\n    \"\"\"\n    Handles missing values in a DataFrame by:\n    - Filling numeric columns with the median (or specified strategy)\n    - Filling categorical columns with a default string (or specified value)\n\n    Parameters:\n    ----------\n    df : pd.DataFrame\n        The DataFrame to clean.\n    numeric_strategy : str or float\n        Strategy for numeric columns: 'median' or a fixed value.\n    categorical_fill : str\n        Value to fill for missing categorical columns.\n\n    Returns:\n    --------\n    pd.DataFrame\n        A cleaned DataFrame with no missing values.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Expected a pandas DataFrame\")\n\n    df_cleaned = df.copy()\n\n    # Fill numeric features\n    numeric_features = df_cleaned.select_dtypes(include=[np.number]).columns\n    for feature in numeric_features:\n        if numeric_strategy == 'median':\n            fill_value = df_cleaned[feature].median()\n        else:\n            fill_value = numeric_strategy\n        df_cleaned[feature] = df_cleaned[feature].fillna(fill_value)\n\n    # Fill categorical features\n    categorical_features = df_cleaned.select_dtypes(include=['object', 'category']).columns\n    for feature in categorical_features:\n        df_cleaned[feature] = df_cleaned[feature].fillna(categorical_fill)\n\n    print(f\"Remaining missing values: {df_cleaned.isnull().sum().sum()}\")\n    return df_cleaned\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:58.016916Z","iopub.execute_input":"2025-04-20T10:45:58.017785Z","iopub.status.idle":"2025-04-20T10:45:58.028434Z","shell.execute_reply.started":"2025-04-20T10:45:58.017755Z","shell.execute_reply":"2025-04-20T10:45:58.027199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Missing values in train_df:\")\nshow_missing_values(train_df)\nprint(\"---------------------------------------------------------\")\nprint(\"Missing values in test_df:\")\nprint(\"---------------------------------------------------------\")\nshow_missing_values(test_df)\ntrain_dataset_no_missing = handle_missing_values(train_df)\ntest_data_no_missing = handle_missing_values(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:58.02931Z","iopub.execute_input":"2025-04-20T10:45:58.02959Z","iopub.status.idle":"2025-04-20T10:45:58.14869Z","shell.execute_reply.started":"2025-04-20T10:45:58.029569Z","shell.execute_reply":"2025-04-20T10:45:58.147446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Target Variable Transformation","metadata":{}},{"cell_type":"markdown","source":"### Visualize the distribution of the target variable 'SalePrice'","metadata":{}},{"cell_type":"code","source":"sns.histplot(train_dataset_no_missing['SalePrice'], kde=True)\nplt.title('Distribution of SalePrice')\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:58.152434Z","iopub.execute_input":"2025-04-20T10:45:58.152714Z","iopub.status.idle":"2025-04-20T10:45:58.692773Z","shell.execute_reply.started":"2025-04-20T10:45:58.152695Z","shell.execute_reply":"2025-04-20T10:45:58.691823Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Histogram Plot Diagnosis:\n\n - The distribution is right-skewed (a long tail to the right).\n\n - Most of the values are clustered around 100,000–200,000.\n\n - There’s no bell-shaped, symmetric curve — a key trait of normal distributions.","metadata":{}},{"cell_type":"markdown","source":"### Log Transformation of SalePrice\n\nThe `SalePrice` variable is not normally distributed — it is right-skewed, as seen in the histogram. Many machine learning models, especially linear regression, assume that the target variable follows a normal distribution.\n\nTo address this, we apply a **logarithmic transformation** to `SalePrice`. This helps to:\n\n- Reduce skewness and bring the distribution closer to normal\n- Compress large values and reduce the influence of outliers\n- Improve the performance and assumptions of regression models\n\nWe use the natural logarithm of (SalePrice + 1) to ensure that zero values, if any, do not cause errors:\n","metadata":{}},{"cell_type":"code","source":"# Apply log(1 + x) to handle 0 values safely\ntrain_dataset_no_missing['SalePrice_log'] = np.log1p(train_dataset_no_missing['SalePrice'])\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\nsns.histplot(train_dataset_no_missing['SalePrice'], kde=True, ax=axs[0])\naxs[0].set_title('Original SalePrice')\n\nsns.histplot(train_dataset_no_missing['SalePrice_log'], kde=True, ax=axs[1])\naxs[1].set_title('Log-Transformed SalePrice')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:58.693899Z","iopub.execute_input":"2025-04-20T10:45:58.69417Z","iopub.status.idle":"2025-04-20T10:45:59.228204Z","shell.execute_reply.started":"2025-04-20T10:45:58.694142Z","shell.execute_reply":"2025-04-20T10:45:59.227384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Effect of Log Transformation on SalePrice Distribution\n\nThe left plot shows the original distribution of the `SalePrice` variable, which is clearly **right-skewed**. This kind of skewness can negatively impact the performance of many machine learning models, especially those that assume a normal distribution of the target variable, such as linear regression.\n\nTo address this, we applied a **log transformation** using `log1p` (log of `SalePrice + 1`). The result is shown in the right plot.\n\nAs seen in the transformed distribution:\n- The shape is now **more symmetric and bell-shaped**, closely resembling a normal distribution.\n- Extreme high values (outliers) are **compressed**, reducing their influence.\n- The transformation helps satisfy the assumptions of normality and homoscedasticity required by several statistical and ML models.\n\nThis transformation prepares the `SalePrice` variable for more reliable and accurate modeling in the next stages.","metadata":{}},{"cell_type":"markdown","source":"## 5. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"After completing the initial exploratory data analysis and cleaning steps, we now apply feature engineering to enhance the dataset.\n\nFeature engineering helps capture domain-specific insights and improves model performance by:\n\n- Creating more informative variables\n- Consolidating redundant features\n- Adding non-linear effects for linear models\n\nBelow are several new features that are meaningful in the context of housing prices:\n- `TotalBathrooms`: Combines full and half bathrooms into a single metric.\n- `HasBasement`: Indicates whether the house has a basement (binary).\n- `HouseAge`: Age of the house since it was built.\n- `Remodeled`: Binary indicator of whether the house has been remodeled.","metadata":{}},{"cell_type":"code","source":"# Create new engineered features\n\n# Total number of bathrooms (assuming HalfBath is in the dataset)\ntrain_dataset_no_missing['TotalBathrooms'] = (\n    train_dataset_no_missing['FullBath'] + 0.5 * train_dataset_no_missing.get('HalfBath', 0)\n)\n\ntest_data_no_missing['TotalBathrooms'] = (\n    test_data_no_missing['FullBath'] + 0.5 * test_data_no_missing.get('HalfBath', 0)\n)\n\n\n# HasBasement: binary feature\ntrain_dataset_no_missing['HasBasement'] = (train_dataset_no_missing['TotalBsmtSF'] > 0).astype(int)\ntest_data_no_missing['HasBasement'] = (test_data_no_missing['TotalBsmtSF'] > 0).astype(int)\n\n# HouseAge: how old the house is at the time of sale\ntrain_dataset_no_missing['HouseAge'] = train_dataset_no_missing['YrSold'] - train_dataset_no_missing['YearBuilt']\ntest_data_no_missing['HouseAge'] = test_data_no_missing['YrSold'] - test_data_no_missing['YearBuilt']\n\n# Remodeled: 1 if the house was remodeled, 0 if not\ntrain_dataset_no_missing['Remodeled'] = (train_dataset_no_missing['YearBuilt'] != train_dataset_no_missing['YearRemodAdd']).astype(int)\ntest_data_no_missing['Remodeled'] = (test_data_no_missing['YearBuilt'] != test_data_no_missing['YearRemodAdd']).astype(int)\n\n# Check the new columns\ntrain_dataset_no_missing[['TotalBathrooms', 'HasBasement', 'HouseAge', 'Remodeled']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:59.22915Z","iopub.execute_input":"2025-04-20T10:45:59.229429Z","iopub.status.idle":"2025-04-20T10:45:59.248821Z","shell.execute_reply.started":"2025-04-20T10:45:59.22941Z","shell.execute_reply":"2025-04-20T10:45:59.247705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. One-Hot Encoding for Categorical Variables","metadata":{}},{"cell_type":"markdown","source":"To generate a complete correlation matrix that includes categorical features, we first need to convert them into a numerical format. Since correlation can only be calculated between numerical variables, we apply **one-hot encoding** to the categorical columns.\n\n**One-hot encoding** creates new binary columns for each category in a categorical feature. This allows us to include these features in correlation analysis.\n\nWe use `pd.get_dummies()` to apply one-hot encoding. The parameter `drop_first=True` is used to avoid multicollinearity by dropping the first category in each feature.\n","metadata":{}},{"cell_type":"code","source":"# One-hot encode categorical variables\ntrain_df_encoded = pd.get_dummies(train_dataset_no_missing, drop_first=True)\ntest_df_encoded = pd.get_dummies(test_data_no_missing, drop_first=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:59.249959Z","iopub.execute_input":"2025-04-20T10:45:59.250222Z","iopub.status.idle":"2025-04-20T10:45:59.334565Z","shell.execute_reply.started":"2025-04-20T10:45:59.250202Z","shell.execute_reply":"2025-04-20T10:45:59.333625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Correlation Heatmap","metadata":{}},{"cell_type":"markdown","source":"#### Focusing on Top Correlated Features\n\nAfter applying one-hot encoding, the number of features in the dataset significantly increased due to the expansion of categorical variables. While this allows for a more complete representation of the data, it also results in a very large and dense correlation matrix that is hard to interpret visually.\n\nTo make the correlation matrix more readable and focused, we limit the analysis to the **top 20 features that are most strongly correlated with `SalePrice`**. This helps us:\n\n- Identify the most important predictors for the target variable\n- Avoid visual clutter from weak or irrelevant correlations\n- Gain clearer insights for feature selection and modeling\n\nWe calculate the absolute correlation values with `SalePrice`, sort them in descending order, and select the top features (excluding `SalePrice` itself). Then, we generate a heatmap based only on these selected features.","metadata":{}},{"cell_type":"code","source":"# Compute absolute correlation with SalePrice_log\ncorr_with_target = train_df_encoded.corr()['SalePrice_log'].abs().sort_values(ascending=False)\n\n# Select top 20 features including SalePrice_log itself\ntop_features = corr_with_target[:21].index  # includes 'SalePrice_log'\n\n# Compute correlation matrix for selected features\ntop_corr = train_df_encoded[top_features].corr()\n\n# Plot heatmap\nplt.figure(figsize=(24, 18))\nsns.heatmap(top_corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\nplt.title('Top 20 Features + SalePrice_log Correlation Heatmap')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:45:59.335619Z","iopub.execute_input":"2025-04-20T10:45:59.335888Z","iopub.status.idle":"2025-04-20T10:46:01.094107Z","shell.execute_reply.started":"2025-04-20T10:45:59.335869Z","shell.execute_reply":"2025-04-20T10:46:01.093108Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Selection Based on Correlation\n\nTo select the most relevant features for modeling, we examined the correlation between each feature and the log-transformed target variable `SalePrice_log`. Features with a high absolute correlation (typically above 0.5) are considered strong predictors.\n\nThe following table summarizes the top features most correlated with `SalePrice_log`, as visualized in the heatmap:\n\n| Feature           | Correlation | Description                                          |\n|------------------|-------------|------------------------------------------------------|\n| OverallQual       | 0.82        | Overall material and finish quality                 |\n| GrLivArea         | 0.70        | Above-ground living area (in square feet)           |\n| GarageCars        | 0.68        | Number of cars that can fit in garage               |\n| GarageArea        | 0.65        | Garage size (in square feet)                        |\n| TotalBathrooms    | 0.65        | Engineered feature: full + 0.5 * half bathrooms     |\n| TotalBsmtSF       | 0.61        | Total basement area                                 |\n| 1stFlrSF          | 0.60        | First floor area                                    |\n| ExterQual_TA      | -0.60       | One-hot encoded: typical/average exterior quality   |\n| FullBath          | 0.59        | Number of full bathrooms                            |\n| HouseAge          | -0.59       | Engineered feature: age of the house                |\n| YearBuilt         | 0.59        | Year the house was originally built                 |\n| YearRemodAdd      | 0.59        | Year of last remodel                                |\n| KitchenQual_TA    | -0.54       | One-hot encoded: typical/average kitchen quality    |\n| TotRmsAbvGrd      | 0.53        | Total rooms above ground (excluding bathrooms)      |\n| Foundation_PConc  | 0.50        | One-hot encoded: foundation made of poured concrete |\n\n---\n\n### Handling Multicollinearity\n\nSome features are highly correlated with each other, which may introduce multicollinearity in linear models. For example:\n\n- `GarageCars` and `GarageArea`: r = **0.88**\n- `TotalBsmtSF` and `1stFlrSF`: r = **0.82**\n- `YearBuilt` and `GarageYrBlt`: r ≈ **0.78**\n\nTo reduce redundancy:\n\n- ✅ **Keep `GarageCars`**, drop `GarageArea` — simpler and more interpretable\n- ✅ **Keep `TotalBsmtSF`**, drop `1stFlrSF` — complements `GrLivArea` better\n\n---\n\n### ✅ Final Selected Features for Modeling\n\nBased on correlation strength and reduced multicollinearity, we select the following features:\n","metadata":{}},{"cell_type":"code","source":"selected_features = [\n    'OverallQual',\n    'GrLivArea',\n    'GarageCars',\n    'TotalBathrooms',\n    'TotalBsmtSF',\n    'FullBath',\n    'ExterQual_TA',\n    'HouseAge',\n    'Foundation_PConc'\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:01.095075Z","iopub.execute_input":"2025-04-20T10:46:01.095343Z","iopub.status.idle":"2025-04-20T10:46:01.09991Z","shell.execute_reply.started":"2025-04-20T10:46:01.095321Z","shell.execute_reply":"2025-04-20T10:46:01.098732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Scatter & Box Plots with Outlier Detection\n","metadata":{}},{"cell_type":"markdown","source":"#### Scatter Plots of Selected Features vs SalePrice_log\n\nTo understand how the selected numeric features relate to the target variable (`SalePrice_log`), we visualize each with a scatter plot. These plots help reveal:\n\n- Linear relationships\n- Outliers or unusual patterns\n- Distribution spread and clustering\n\nNote: The binary feature `ExterQual_TA` is not included in the scatter plots since it's categorical. Instead, we use a box plot for it.\n","metadata":{}},{"cell_type":"code","source":"# Scatter plots for numeric features only\nplt.figure(figsize=(18, 20))\ni = 1\nfor feature in selected_features:\n    if feature in train_df.columns and pd.api.types.is_numeric_dtype(train_df[feature]):\n        plt.subplot(4, 2, i)\n        sns.scatterplot(data=train_df_encoded, x=feature, y='SalePrice_log', alpha=0.6)\n        plt.title(f'{feature} vs SalePrice_log')\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice_log')\n        i += 1  # Only increment subplot index for valid features\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:01.101059Z","iopub.execute_input":"2025-04-20T10:46:01.101946Z","iopub.status.idle":"2025-04-20T10:46:02.291143Z","shell.execute_reply.started":"2025-04-20T10:46:01.101909Z","shell.execute_reply":"2025-04-20T10:46:02.290404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Interpretation of Scatter Plots\n\nThe following scatter plots visualize the relationship between the selected numeric features and the target variable `SalePrice_log`. These plots help us assess feature strength, linearity, outliers, and reliability as predictors in a regression model.\n\n---\n\n##### 🔹 OverallQual\n- Strong **positive and stepwise** relationship with `SalePrice_log`.\n- Higher quality clearly leads to higher prices.\n- Clean, ordinal feature with minimal noise — very reliable.\n\n---\n\n##### 🔹 GrLivArea (Above-ground living area)\n- Shows a strong **positive linear trend**.\n- As expected, larger living areas correspond to higher sale prices.\n- A few high-area values deviate slightly downward — potential mild outliers.\n\n---\n\n##### 🔹 GarageCars\n- Discrete and clear **stepwise positive** trend.\n- Price increases steadily with garage capacity (up to ~3–4 cars).\n- Great candidate for tree models, but could also be treated as categorical.\n\n---\n\n##### 🔹 TotalBathrooms (Engineered Feature)\n- General upward trend with increasing bathrooms.\n- Slight scatter within each bathroom level, but relationship remains **visibly positive**.\n- Effectively consolidates `FullBath` and `HalfBath` into one strong feature.\n\n---\n\n##### 🔹 TotalBsmtSF\n- Strong **positive linear** pattern.\n- Some low-priced homes with large basements indicate mild outliers or special cases.\n- Zero values clearly show homes without basements — may benefit from a `HasBasement` feature.\n\n---\n\n##### 🔹 FullBath\n- Clear stepwise trend, though more **scatter** than `TotalBathrooms`.\n- Visual support for combining with half baths into the engineered `TotalBathrooms`.\n\n---\n\n##### 🔹 HouseAge (Engineered Feature)\n- Displays a **negative relationship**: older homes tend to have slightly lower prices.\n- Some scatter in older homes but still a visible trend.\n- Offers valuable historical context to complement physical attributes.\n\n---\n\n### 📌 Summary\n\nMost features show a **clear, interpretable relationship** with the target variable. \n- Features like `OverallQual`, `GrLivArea`, `TotalBsmtSF`, and `GarageCars` exhibit strong predictive power.\n- Engineered features such as `TotalBathrooms` and `HouseAge` add interpretability and structure.\n- Some minor outliers are visible but do not overwhelm the trends — handling them strengthens model performance.\n\nThese features are all strong candidates for inclusion in a predictive regression model.\n","metadata":{}},{"cell_type":"markdown","source":"#### Outlier Removal\n\nBefore visualizing the final scatter plots, we applied **outlier removal** using the **Interquartile Range (IQR)** method to clean key numeric features. This ensures the relationships between features and the target variable `SalePrice_log` are not distorted by extreme values.\n\nWe focused on features that showed clear predictive value and potential outliers:\n- `GrLivArea` (above-ground living area)\n- `TotalBsmtSF` (basement area)\n- `SalePrice_log` (target variable)\n\nOutliers were defined as values outside the range:\n- **[Q1 − 1.5 × IQR, Q3 + 1.5 × IQR]**, where Q1 and Q3 are the 25th and 75th percentiles\n\n---\n\nRemoving outliers helps:\n- Improve the interpretability of visualizations\n- Prevent skewed regression coefficients\n- Reduce variance and enhance generalization during model training\n\nThis preprocessing step ensures that the model learns from the **core distribution** of typical homes, without being biased by rare or extreme cases.\n","metadata":{}},{"cell_type":"code","source":"def remove_outliers_iqr(df, features):\n    cleaned_df = df.copy()\n    for feature in features:\n        q1 = cleaned_df[feature].quantile(0.25)\n        q3 = cleaned_df[feature].quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n        # Keep only values within the IQR range\n        cleaned_df = cleaned_df[(cleaned_df[feature] >= lower_bound) & (cleaned_df[feature] <= upper_bound)]\n        print(f\"{feature}: Removed {(len(df) - len(cleaned_df))} outliers\")\n        df = cleaned_df  # Update reference for chained filtering\n    return cleaned_df\n\n# Features to check\noutlier_features = ['GrLivArea', 'TotalBsmtSF', 'SalePrice_log']\n\n# Apply outlier removal\ntrain_dataset_no_missing = remove_outliers_iqr(train_df_encoded, outlier_features)\n\n# Update the encoded DataFrame accordingly\ntrain_encoded_df_cleaned = train_df_encoded.loc[train_dataset_no_missing.index]\n\nprint(\"Original dataset shape:\", train_df.shape)\nprint(\"Cleaned dataset shape:\", train_dataset_no_missing.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:02.293034Z","iopub.execute_input":"2025-04-20T10:46:02.293378Z","iopub.status.idle":"2025-04-20T10:46:02.31513Z","shell.execute_reply.started":"2025-04-20T10:46:02.293345Z","shell.execute_reply":"2025-04-20T10:46:02.313791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_to_compare = ['GrLivArea', 'TotalBsmtSF']\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(18, 10))\n\nfor i, feature in enumerate(features_to_compare):\n    # Before cleaning\n    sns.scatterplot(x=train_df_encoded[feature], y=train_df_encoded['SalePrice_log'], alpha=0.5, ax=axs[i][0])\n    axs[i][0].set_title(f'{feature} vs SalePrice_log (Before Cleaning)')\n    axs[i][0].set_xlabel(feature)\n    axs[i][0].set_ylabel('SalePrice_log')\n\n    # After cleaning\n    sns.scatterplot(x=train_dataset_no_missing[feature], y=train_dataset_no_missing['SalePrice_log'], alpha=0.5, ax=axs[i][1])\n    axs[i][1].set_title(f'{feature} vs SalePrice_log (After Cleaning)')\n    axs[i][1].set_xlabel(feature)\n    axs[i][1].set_ylabel('SalePrice_log')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:02.316227Z","iopub.execute_input":"2025-04-20T10:46:02.31655Z","iopub.status.idle":"2025-04-20T10:46:03.170344Z","shell.execute_reply.started":"2025-04-20T10:46:02.316522Z","shell.execute_reply":"2025-04-20T10:46:03.169279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Conclusion: Effect of Outlier Removal\n\nThe scatter plots above compare the relationship between key numerical features (`GrLivArea` and `TotalBsmtSF`) and the target variable (`SalePrice_log`) before and after outlier removal.\n\n##### Key Observations:\n\n- **Before cleaning**, the data contained a small number of extreme values (e.g., `GrLivArea > 4000`, `TotalBsmtSF > 4000`) that did not follow the overall linear trend.\n- These outliers appeared as isolated points, especially in the upper-right corner of the plots, potentially distorting model coefficients and reducing generalizability.\n- **After cleaning**, the relationships between the features and `SalePrice_log` became more clearly linear, tightly clustered, and easier to model.\n- The resulting data shows **less noise and better-defined patterns**, which is ideal for building regression models.\n\n##### Conclusion:\n\nRemoving outliers improved the data quality and will likely result in a more accurate and stable predictive model. The cleaned dataset provides a more consistent representation of the underlying relationship between living area, basement size, and house prices.\n","metadata":{}},{"cell_type":"markdown","source":"### Box Plots for One-Hot Encoded Categorical Features\n\nFor categorical features that were one-hot encoded (i.e., converted to binary columns), scatter plots aren't informative. Instead, we use **box plots** to visualize how the presence (1) or absence (0) of each category affects the distribution of `SalePrice_log`.\n\nBox plots allow us to:\n- Compare median and spread of `SalePrice_log` for different categories\n- Identify which categories are associated with higher or lower sale prices\n- Spot outliers in each category group\n\nBelow are box plots for the selected one-hot encoded categorical features.\n","metadata":{}},{"cell_type":"code","source":"# List of one-hot encoded binary features we selected\nencoded_features = ['ExterQual_TA']  # Add more if needed\n\n# Plot boxplots\nplt.figure(figsize=(8, 5 * len(encoded_features)))\n\nfor i, feature in enumerate(encoded_features, 1):\n    plt.subplot(len(encoded_features), 1, i)\n    sns.boxplot(x=train_dataset_no_missing[feature], y=train_dataset_no_missing['SalePrice_log'])\n    plt.title(f'{feature} vs SalePrice_log')\n    plt.xlabel(f'{feature} (0 = absence, 1 = presence)')\n    plt.ylabel('SalePrice_log')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:03.171265Z","iopub.execute_input":"2025-04-20T10:46:03.171526Z","iopub.status.idle":"2025-04-20T10:46:03.357899Z","shell.execute_reply.started":"2025-04-20T10:46:03.171505Z","shell.execute_reply":"2025-04-20T10:46:03.356943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Box Plot Analysis: `ExterQual_TA` (One-Hot Encoded Feature)\n\n`ExterQual_TA` is a one-hot encoded binary feature indicating whether a house has a **Typical/Average** exterior quality.\n\n- `0` → Exterior quality is **not** 'TA' (could be Excellent, Good, Fair, etc.)\n- `1` → Exterior quality **is** 'TA'\n\nThe box plot shows that:\n- Houses with `ExterQual_TA = 1` (Typical quality) generally have **lower sale prices**\n- Houses with other exterior quality levels (`ExterQual_TA = 0`) tend to have **higher sale prices**\n\nThis confirms that **exterior quality is a meaningful categorical feature**, and even a single one-hot encoded level like `ExterQual_TA` provides valuable information for predicting housing prices.\n","metadata":{}},{"cell_type":"markdown","source":"### Now that we’ve prepared and cleaned the data, we proceed to model training using the selected features.","metadata":{}},{"cell_type":"code","source":"train = train_encoded_df_cleaned.copy()\ntest = test_df_encoded.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:03.358776Z","iopub.execute_input":"2025-04-20T10:46:03.359018Z","iopub.status.idle":"2025-04-20T10:46:03.366766Z","shell.execute_reply.started":"2025-04-20T10:46:03.358997Z","shell.execute_reply":"2025-04-20T10:46:03.365807Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Model Training & Evaluation","metadata":{}},{"cell_type":"markdown","source":"This section includes:\n- Final model training on the full training data\n- Evaluation on a validation split from the training data\n- Prediction on the official test set\n- Submission file creation for Kaggle","metadata":{}},{"cell_type":"markdown","source":"### 🔹 9.1 Final Model Training (All Data)","metadata":{}},{"cell_type":"code","source":"t_train = train['SalePrice_log']\nX_train = train[selected_features]\nX_test = test[selected_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:03.367685Z","iopub.execute_input":"2025-04-20T10:46:03.368448Z","iopub.status.idle":"2025-04-20T10:46:03.384022Z","shell.execute_reply.started":"2025-04-20T10:46:03.368415Z","shell.execute_reply":"2025-04-20T10:46:03.383004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train on the full training set\nNE_model = linear_model.LinearRegression()\nNE_model.fit(X_train, t_train)\n\n# Predict log-transformed sale prices on test data\ny_pred_log = NE_model.predict(X_test)\n\n# Convert back to original sale price scale\ny_pred = np.expm1(y_pred_log)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:03.384971Z","iopub.execute_input":"2025-04-20T10:46:03.385285Z","iopub.status.idle":"2025-04-20T10:46:03.428022Z","shell.execute_reply.started":"2025-04-20T10:46:03.385259Z","shell.execute_reply":"2025-04-20T10:46:03.427052Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 9.2 Model Evaluation (Train/Validation Split)\n\nBefore submitting, it's good practice to evaluate the model's performance on a validation set.\n\nWe split the training data into:\n- 80% for training\n- 20% for validation\n\nWe then evaluate the model using:\n- R² score\n- RMSE in both log and original scales\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create validation split\nX_train_sub, X_valid, t_train_sub, t_valid = train_test_split(X_train, t_train, test_size=0.2, random_state=42)\n\n# Train on subset\neval_model = linear_model.LinearRegression()\neval_model.fit(X_train_sub, t_train_sub)\n\n# Predict on validation set\nt_pred_valid = eval_model.predict(X_valid)\n\n# Evaluation in log scale\nr2 = r2_score(t_valid, t_pred_valid)\nrmse_log = metrics.mean_squared_error(t_valid, t_pred_valid, squared=False)\n\nprint(\"R² Score (log space):\", round(r2, 4))\nprint(\"RMSE (log space):\", round(rmse_log, 4))\n\n# Convert back to original scale\nt_valid_actual = np.expm1(t_valid)\nt_pred_actual = np.expm1(t_pred_valid)\n\nrmse_original = metrics.mean_squared_error(t_valid_actual, t_pred_actual, squared=False)\nprint(\"RMSE (original scale):\", round(rmse_original, 2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:03.429111Z","iopub.execute_input":"2025-04-20T10:46:03.429652Z","iopub.status.idle":"2025-04-20T10:46:03.448473Z","shell.execute_reply.started":"2025-04-20T10:46:03.429621Z","shell.execute_reply":"2025-04-20T10:46:03.447157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 9.3 Submission File Creation\n\nFinally, we create the submission file using the predicted sale prices (converted back from log scale).","metadata":{}},{"cell_type":"code","source":"# Create submission DataFrame\nsubmission = pd.DataFrame({\n    'Id': test['Id'],\n    'SalePrice' : y_pred\n})\n\n# Export to CSV\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file saved as submission.csv in the data directory.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T10:46:03.449649Z","iopub.execute_input":"2025-04-20T10:46:03.450004Z","iopub.status.idle":"2025-04-20T10:46:03.470311Z","shell.execute_reply.started":"2025-04-20T10:46:03.449973Z","shell.execute_reply":"2025-04-20T10:46:03.468847Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 9.4 Training SGDRegressor Using a Pipeline\n\nTo create a clean and standardized model training workflow, we use a `Pipeline` that combines:\n\n- **StandardScaler()**: to normalize the features to zero mean and unit variance\n- **SGDRegressor()**: an iterative linear model that optimizes via gradient descent\n\nThis approach improves training stability and aligns with best practices for models that require scaled input.\n\nWhile the final predictions were made using the Normal Equation (LinearRegression), this SGD pipeline shows that we can reach comparable performance with an iterative method — and it's useful for situations with large-scale data or online learning.\n\nBelow we evaluate its performance using R² and RMSE on both training and validation sets.","metadata":{}},{"cell_type":"code","source":"# Split data for validation\nX_train_sub, X_valid, y_train_sub, y_valid = model_selection.train_test_split(X_train, t_train, test_size=0.2, random_state=42)\n\n# Initialize scaler and model\nscaler = preprocessing.StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_sub)\nX_valid_scaled = scaler.transform(X_valid)\n\nsgd_model = linear_model.SGDRegressor(\n    loss='squared_error',\n    learning_rate='constant',\n    eta0=0.001,\n    alpha=0.0001,\n    penalty='l2',\n    max_iter=1,\n    warm_start=True,\n    random_state=42\n)\n\n# Track RMSE over epochs\ntrain_rmse = []\nvalid_rmse = []\n\nfor epoch in range(200):\n    sgd_model.fit(X_train_scaled, y_train_sub)\n\n    y_train_pred = sgd_model.predict(X_train_scaled)\n    y_valid_pred = sgd_model.predict(X_valid_scaled)\n\n    train_rmse.append(mean_squared_error(y_train_sub, y_train_pred,squared=False))\n    valid_rmse.append(mean_squared_error(y_valid, y_valid_pred, squared=False))\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(train_rmse, label='Train RMSE', color='blue')\nplt.plot(valid_rmse, label='Validation RMSE', color='orange')\nplt.xlabel('Epoch')\nplt.ylabel('RMSE (log space)')\nplt.title('RMSE Over Epochs Using SGDRegressor (Standardized)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:46:39.493745Z","iopub.execute_input":"2025-04-20T10:46:39.494071Z","iopub.status.idle":"2025-04-20T10:46:40.096987Z","shell.execute_reply.started":"2025-04-20T10:46:39.494045Z","shell.execute_reply":"2025-04-20T10:46:40.096031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data for evaluation\nX_train_sub, X_valid, t_train_sub, t_valid = model_selection.train_test_split(X_train, t_train, test_size=0.2, random_state=42)\n\n# Create pipeline: StandardScaler + SGDRegressor\nsgd_pipeline = pipeline.make_pipeline(\n    preprocessing.StandardScaler(),\n    linear_model.SGDRegressor(\n        loss='squared_error',\n        alpha=0.0001,        # Regularization strength (L2)\n        learning_rate='invscaling',\n        eta0=0.01,\n        max_iter=1000,\n        tol=1e-3,\n        random_state=42\n    )\n)\n\n# Train the pipeline\nsgd_pipeline.fit(X_train_sub, t_train_sub)\n\n# Predict on train and validation sets\nt_pred_train = sgd_pipeline.predict(X_train_sub)\nt_pred_valid = sgd_pipeline.predict(X_valid)\n\n# Evaluate\nprint(\"R² score on train:\", metrics.r2_score(t_train_sub, t_pred_train))\nprint(\"R² score on validation:\", metrics.r2_score(t_valid, t_pred_valid))\nprint()\n\nprint(\"RMSE on train:\", mean_squared_error(t_train_sub, t_pred_train, squared=False))\nprint(\"RMSE on validation:\", mean_squared_error(t_valid, t_pred_valid, squared=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:47:23.064133Z","iopub.execute_input":"2025-04-20T10:47:23.064489Z","iopub.status.idle":"2025-04-20T10:47:23.088166Z","shell.execute_reply.started":"2025-04-20T10:47:23.064464Z","shell.execute_reply":"2025-04-20T10:47:23.087154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Final Summary\n\nThis project applied a complete machine learning workflow to the Kaggle House Prices dataset, focusing on regression using both closed-form and iterative optimization approaches.\n\n### ✅ Key Steps Completed:\n- Performed in-depth **Exploratory Data Analysis (EDA)** and visualizations\n- Handled **missing values**, removed **outliers**, and applied meaningful **feature engineering**\n- Transformed the target variable (`SalePrice`) using **log transformation** for better modeling\n- Conducted **feature selection** based on correlation and interpretability\n- Trained a linear regression model using the **Normal Equation** (closed-form solution)\n- Created a **submission file** with predictions on the test set in original price scale\n- Evaluated model performance using **R² and RMSE** on a validation split\n- Visualized **RMSE over epochs** using **SGDRegressor** with standardized features to satisfy iterative learning requirements\n- Implemented a clean, production-ready pipeline using `StandardScaler + SGDRegressor`\n\n### 🔍 Final Model Choice:\nThe final submission uses the **Normal Equation-based model**, which achieved strong generalization and simplicity, making it well-suited for this dataset size.\n\nThe **SGDRegressor** was used for:\n- Learning curve visualization (required by assignment)\n- Exploring iterative training behavior\n\n### 📈 Evaluation Highlights:\n- **Linear Regression R² (Validation):** ~0.80\n- **SGDRegressor R² (Validation):** ~0.80\n- **RMSE (Log space):** ~0.14–0.15  \n- **Clean convergence curve over 200 epochs**\n\nThis notebook demonstrates both technical depth and clear structure, with a solid understanding of regression modeling and good ML practices.\n","metadata":{}}]}